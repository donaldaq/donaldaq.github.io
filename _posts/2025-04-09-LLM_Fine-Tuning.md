---
layout: post
title: 효율적인 파라미터 미세조정기법 for LLM
excerpt: "LLM Fine-Tuning"
categories: [LLM]
comments: true
---

**Image DeepLearning Engineer입니다. AI 모든 분야에 관심이 많습니다. 
NLP 연구개발 공부하고 관심갖는 사람으로서 논문과 정리된 내용을 좀 더 쉽게 이해하는데 집중하여 작성하였습니다.**

### 논문 요약

이 논문은 대규모 AI 모델을 효율적으로 미세조정하는 PEFT(Parameter-Efficient Fine-Tuning) 기법에 대해 다룹니다. 

주요 내용:

1. 핵심 문제: GPT-3와 같은 대형 모델의 전체 미세조정은 막대한 계산 비용과 저장 공간이 필요
2. PEFT의 목적: 최소한의 파라미터 조정으로 모델의 성능은 유지하면서 비용을 크게 절감
3. 주요 방법론:
    - 선택적 미세조정 - 일부 파라미터만 선택적으로 조정
    - 어댑터 기반 미세조정 - 작은 추가 모듈을 삽입하여 특정 작업에 적응
    - 프롬프트 미세조정 - 입력과 레이어에 프롬프트를 통합하여 조정
    - 재매개변수화 미세조정 - 모델의 일부 파라미터를 변형하여 학습
4. 효과: LoRA와 같은 기법은 GPT-3의 파라미터 중 0.1%만 사용하면서도 성능 저하는 0.1~0.5%에 불과

![Image](https://github.com/user-attachments/assets/246e9ec3-1c80-46a5-bf9f-45995d1452da)

## LLM모델의 Fine-tuning 문제

현재 서비스되고 있는 AI모델들은 거대해지고 있어 특정 목적에 맞게 조정하려면 엄청난 연산 비용과 저장 공간이 필요합니다. GPT-3같은 모델은 1750억 개의 파라미터를 가지고 있어, 이를 Fine-tuning하는 것은 거의 불가능하다  

이 논문은 이런 문제를 해결하기 위해 "Parameter-Efficient Fine-Tuning (PEFT)", 즉 효율적인 파라미터 미세 조정 기법을 정리하고 분석한 내용을 담고 있다. 

핵심 아이디어는 전체 모델을 조정하지 않고도 성능을 유지하면서 비용을 줄이는 방법을 찾는 방법을 제안합니다. 

PEFT 기법은 적은 연산량으로 강력한 모델 성능을 유지할 수 있어서, 최근 LLaMA, Qwen, DALL-E 같은 모델들에서도 활발하게 활용되고 있다고 합니다. 

이 논문은 PEFT의 

- 핵심 개념
- 주요 방법론
- 최신 트렌드 및 응용 사례까지

폭넓게 다루고 있어, 이 분야를 처음 접하는 사람부터 전문가까지 모두에게 유용한 가이드 역할이 될 것 같습니다. 

### 왜 PEFT가 필요한가?

기존의 모델 미세 조정(fine-tuning) 방식은 두 가지 주요 문제를 가지고 있습니다. 

1. 막대한 연산 비용

- GPT-3 같은 대형 모델을 미세 조정하려면 수천 개의 GPU가 필요합니다.
- 예를 들어, GPT-3의 모든 1750억 개의 파라미터를 조정하는 것은 막대한 비용과 시간이 소요되고현실적으로 많은 기업과 연구팀이 이런 비용을 감당할 수 없음.

2. 전이 학습(Transfer Learning)의 한계

- 기존 미세 조정 방식은 특정 작업에서만 유용하고, 다른 작업에는 다시 학습이 필요합니다. 특정 목적과 도메인에 맞춰 학습해야 하고 학습된 결과의 성능을 보장할 수 없습니다.

**PEFT는 이 두 가지 문제를 해결하기 위해 등장**했습니다. 적은 연산량으로도 효과적으로 모델을 조정하고, 다양한 작업이 가능하도록 합니다

![Image](https://github.com/user-attachments/assets/d9653ce8-665c-4b5d-8fce-e9df94d064e1)

### PEFT의 핵심 개념과 주요 기법

PEFT를 5가지 핵심 방법으로 분류하고 있습니다.

1. 선택적 미세 조정(Selective Fine-Tuning)

- 기존 모델의 일부 파라미터만 조정하는 방식.
    - Examples:
        - Layer Freezing (레이어 고정) – 모델의 일부 층만 조정하여 연산량 절약하는 방법.
        - BitFit – 모델의 바이어스(bias) 값만 조정하여 최소한의 조정으로 효과적으로 미세조정.
    - 주요 장점: 최소한의 연산량으로 미세 조정 가능.

2. 어댑터 기반 미세 조정(Adapter Tuning)

- 기존 모델에 **작은 추가 모듈(어댑터)**을 삽입하여 특정 작업에 적응하는 방식입니다.
    - Examples:
        - LoRA (Low-Rank Adaptation) – 모델의 일부 파라미터만 조정하여 성능 유지하는 방법.
        - MAD-X – 다국어 NLP 모델에서 특정 언어에 맞춰 어댑터 추가하는 방법.
    - 주요 장점: 모델의 원래 성능을 유지하면서도 새로운 작업에 적응 가능.

3. 프롬프트 미세 조정(Prompt Tuning)

- 프롬프트(입력)를 학습하여 모델이 원하는 방식으로 도출하는 방법입니다.
    - Examples:
        - Prefix Tuning – 모델의 입력 앞에 학습된 프롬프트를 추가하여  성능 개선을 기대하는 방법.
        - P-Tuning – 학습 가능한 프롬프트를 활용하여 다양한 작업에 모델을 쉽게 적용하는 방법.
    - 주요 장점: 모델의 파라미터를 거의 변경하지 않고도 특정 작업에 맞출 수 있음.

4. 재매개변수화 미세 조정(Reparameterization PEFT)

- 기존 모델의 일부 파라미터를 변형하여 학습하는 방식입니다.
    - Examples:
        - LoRA – 모델의 일부 레이어만 미세 조정하는 방식으로 연산량을 99% 이상 절약.
        - QLoRA – LoRA를 4비트 양자화하여 GPU 메모리 사용을 더욱 줄입니다
    - 주요 장점: 낮은 연산량으로 기존 모델을 효율적으로 활용 가능.

5. 하이브리드 PEFT(Hybrid PEFT)

- 여러 가지 PEFT 방법을 조합하여 최적의 성능을 내는 방식입니다.
    - Examples:
        - UniPELT – LoRA, Adapter, Prefix Tuning을 결합하여 성능 향상을 합니다.
        - NOAH – 프롬프트, 어댑터, LoRA 등을 조합하여 최적의 조정 방식 탐색합니다.
        - 주요 장점: 각 PEFT 기법의 장점을 결합하여 최상의 성능 제공.

![Image](https://github.com/user-attachments/assets/63a17aaa-67f8-4d48-bddf-2d2d553fbc44)

### PEFT의 실험 결과와 실제 활용 사례

논문에서는 다양한 실험을 통해 PEFT가 기존 방법보다 효율적이라는 것을 증명해줍니다. 

1. LoRA는 기존 풀 파인튜닝(Full Fine-Tuning) 대비 GPU 메모리를 99% 절약하면서도 성능은 0.1~0.5%만 감소하여 최소한의 손실만 기대합니다. 
2. QLoRA는 단 8GB의 VRAM으로도 GPT-3 같은 대형 모델을 미세 조정할 수 있도록 합니다.
3. 프롬프트 기반 미세 조정(Prompt Tuning)은 기존 GPT-3의 Zero-shot 성능을 훨씬 향상시켜주고  PEFT는 다양한 실제 응용 사례에서 강력한 성능을 입증했습니다. 
4. 챗봇(ChatGPT, Qwen): LoRA와 프롬프트 튜닝을 활용하여 새로운 대화 스타일을 학습합니다.
5. 이미지 생성(DALL-E, Stable Diffusion): ControlNet과 같은 어댑터를 활용하여 특정 스타일의 이미지 생성도 합니다. 
6. 자율주행 및 비전 모델(SAM, Grounding DINO): 시각 모델에서 특정 물체를 탐색할 때 효율적인 미세 조정 기법으로 활용합니다.

### PEFT는 미래 AI 모델 개발의 핵심 기술

PEFT는 대형 AI 모델의 비용, 연산량, 확장성 문제를 해결할 수 있는 중요한 기술이 될 수 있습니다. 특히 LoRA, Adapter, Prompt Tuning 등은 적은 자원으로도 대형 모델을 활용할 수 있도록 도와주며, 앞으로 더 많은 연구와 산업 적용을 기대할 수 있습니다. 

기존 모델을 그대로 성능 유지하면서도 최소한의 조정으로 원하는 목적에 부합하는 수행을 할 수 있는 PEFT 기법은 AI 연구자, 기업, 스타트업 모두에게 필요한 도구가 될 것으로 기대합니다. 

이 논문은 PEFT의 개념부터 최신 트렌드까지 정리한 종합 가이드인 논문으로 이 분야에 흥미가 있다면 읽어볼 필요가 있을 것 같습니다 .

### Reference:
- Parameter-Efficient Fine-Tuning for Foundation Models



